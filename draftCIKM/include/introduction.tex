\section{Introduction}
%So far, the focus of this thesis has targeted classic web data extraction: we extract entities from semi-structured web sources, along with their attributes, in order to store them in some structured format. A novel research direction is recently focusing on extracting data from web documents to construct knowledge bases (KBs), in order to structurally store entities and relationships among them. Given the different nature of target data, KBs are usually not stored in relational databases. In this chapter we move the attention from classic web data extraction to KBs built from web sources. More specifically, we consider RDF KBs, the most popular format where data is stored as RDF triples.

Building large 
%RDF%
knowledge-bases (KBs) has become a popular trend in information extraction.
KBs store information in the form of triples, where a \emph{predicate}, or relation, expresses a binary relation between a \emph{subject} and a \emph{object}. KB triples, called facts, store information about real-world entities and their relationships, such as %``Scott Eastwood is the child of Clint Eastwood",
``Michelle Obama is married to Barack Obama", or ``Larry Page is the founder of Google".
Significant effort has been put on KBs creation in the last 10 years in the research community (\system{DBPedia}~\cite{bizer2009dbpedia}, \system{FreeBase}\cite{bollacker2008freebase}, \system{Wikidata}~\cite{vrandevcic2014wikidata}, \deepdive~\cite{shin2015incremental}, \system{Yago}~\cite{suchanek2007yago}, 
%\system{NELL}~\cite{carlson2010toward}, 
\system{TextRunner}~\cite{banko2007open}) as well as in the industry % and KBs have also attracted interests from the industry, as evident from the ongoing efforts in several companies 
(e.g., 
%Facebook
%\footnote{\url{https://developers.facebook.com/docs/graph-api}}, 
Google~\cite{dong2014data}, Wal-Mart~\cite{deshpande2013building}).

Unfortunately, due to their creation process, KBs are usually erroneous and incomplete.
KBs are bootstrapped by extracting information from sources 
%%%%%%%%%JOURNAL% oftentimes from the Web, 
with minimal or no human intervention.  This leads to two main problems. First, false facts in the KB are propagated from the sources, or introduced by the extractors. Second, usually KBs do not limit the information of interest with a schema
% that clearly defines instance data. The set of predicates is unknown a-priori, 
and let users add facts defined on new predicates by simply inserting new triples. % without any integrity check.  
Since \emph{closed world assumption} (CWA) does not longer hold in KBs ~\cite{dong2014data,galarraga2015fast}, we cannot assume that a missing fact is false %%%%%%%%%JOURNAL% , but rather we should label it as \emph{unknown} 
(\emph{open world assumption}).

As a %%%%%%%%%JOURNAL%direct 
consequence, the amount of errors and incompleteness in KBs is usually significant, with up to 30\% errors for facts derived from the Web% is significantly larger than in classic databases
~\cite{abedjan2015temporal,suchanek2009sofie}.
Since KBs are large, e.g., \wikidata has more than $1$B facts and $300$M different entities, %\footnote{\url{https://query.wikidata.org/}}. 
checking all triples to find errors or to add new facts cannot be done manually. % and tools are needed to assist humans in the KBs curation. 
A natural approach to assist curators of KBs is to discover %However, KBs carry such a big amount of information that they can be mined to discover 
\emph{declarative rules} that can be executed over the KB to improve the quality of the data~\cite{Chen:2016,abedjan2014amending,galarraga2015fast}. 
%%%%%%%%%JOURNAL%However, these approaches have focused on the discovery of rules to derive new facts, while %%%%%%%%%JOURNAL% for the first time
% , and this is the problem we study in
%inspected to find additional information or constraints over existing data. 
%this paper.
% we investigate the problem of discovering \emph{declarative rules} over RDF KBs. 
%More specifically, 
In this paper we target the discovery %of first-order Horn Rules constrained by some language biases, introducing 
of two different types of rules:
\begin{inparaenum}[\itshape(i)]
	\item {\em positive rules} to enrich the KB with new facts and thus increase its coverage; %%%%%%%%%JOURNAL% of the reality;
	\item {\em negative rules} to spot logical inconsistencies and identify erroneous triples.
\end{inparaenum}

\begin{example}\label{ex:krd_intro}
	Consider a KB with information about parent and child relationships.
	A positive rule $r_1$ can be:
	
	\vspace{-2ex}
	{\small
		\begin{equation*}
			\atom{parent}{b}{a} \Rightarrow \atom{child}{a}{b}
		\end{equation*}
	} 
	\vspace{-2ex}
	
	\noindent
	It states that if a person $a$ is parent of person $b$, then $b$ is child of $a$. 
	%If the KB has the fact ``Clint Eastwood is the parent of Scott Eastwood", we can infer the fact that ``Scott Eastwood is the child of Clint Eastwood".
	A negative rule $r_2$ has similar form, but different semantics ({\em DOB} stands for Date Of Birth):
	
	\vspace{-2ex}
	{\small
		\begin{equation*}
			\atom{DOB}{a}{v_0} \wedge \atom{DOB}{b}{v_i} \wedge v_0 > v_i  \Rightarrow \neg \atom{child}{a}{b}
		\end{equation*}
	} 
	\vspace{-2ex}
	
	\noindent It states that person $b$ cannot be child of $a$ if $a$ was born after $b$. By instantiating the rule for \texttt{child} facts, we identify erroneous triples stating that a child is born before a parent.
\end{example}

While intuitive to humans, the rules above must be manually stated in a KB to be enforced, and there are thousands of rules in a large KB~\cite{gc2015big}.
%\myparagraph{Novelty} In this chapter 
%\vspace{1ex}
A rule discovery system should therefore discover both positive and negative rules. 
%However, this comes with a computational cost, as the search space quickly becomes much larger. For this reason, existing approaches for mining positive rules prune aggressively the search space, and rely on a simple language% without values comparisons
%~\cite{Chen:2016,abedjan2014amending,galarraga2015fast}.
%MAKE SURE TO CITE  the demo DBLP:conf/sigmod/FaridRIHC16, they discover negative rules, but there is not paper, therefore no baseline%%%%%%%%%%
%%%%%%%%%JOURNAL% with hundreds of predicates 
%%%%%%%%%JOURNAL% Other than enriching and cleaning KBs, negative rules support other use cases. %brings multiple benefits. %, such as automatic reasoning. 
%We can discover negative facts (absent in KBs), bringing the KB world a step closer to the classic database CWA. 
%%%%%%%%%JOURNAL% We will show how such rules 
%can help domain experts in automatically defining rules to maintain and curate Big Data industrial systems. We 
%%%%%%%%%JOURNAL% can improve Machine Learning tasks by providing meaningful training examples~\cite{richardson2006markov,shin2015incremental}.
%
%MOVE TO RELATED WORK SECTION %%%%%%%
%While there are been several papers addressing rules discovery in databases, efforts on discovery of rule for KBs are relatively recent. Discovering declarative rules from data sources is a long standing research challenge. Inductive Logic Programming (ILP) was the first discipline to mine Horn Rules from background knowledge and training examples~\cite{muggleton1994inductive}. Similarly, a significant body of work has addressed the problem of discovering constraints over relational databases. The most popular constraint families are Functional Dependencies ~\cite{abiteboul1995foundations,huhtala1999tane,wyss2001fastfds} and Denial Constraints~\cite{chu2013discovering}.
%
%\vspace{1mm}
However, %this is the first work that address the discovery of both positive and negative rules for KBs, thus facing 
three main challenges arise when discovering rules for KBs.

\noindent \myparagraph{Errors}
Traditional techniques for rule discovery assume that data is either clean or has a negligible amount of errors~\cite{abiteboul1995foundations,chu2013discovering}. We will show that KBs present a high percentage of errors and we need techniques that are noise tolerant.

\noindent \myparagraph{Open World Assumption}
Other approaches rely on the presence of positive and negative examples~\cite{dehaspe1999discovery,muggleton1994inductive}, but KBs contain only positive statements, and, without CWA, there is no immediate solution to derive %negative statements as 
counter examples.
% --   or a fixed schema given as input.  

%REFERENCE AND/OR EXPLANATION NEEDED HERE wrt the POS NEG EXAMPLES , if the ONE I PUT OK?
% STEFANO: ADDED A FURTHER REFERENCE

\noindent \myparagraph{Volume}
%MOVE TO RELATED
%Classic database discovery algorithms ILP systems cannot cope with large KBs and in order to directly apply existing database techniques would require the materialisation of all possible predicates combinations into relational tables (graph data must be flattened in relational format).  Moreover, 
Existing approaches for rule discovery assume that data fit into main memory~\cite{abedjan2014amending,galarraga2015fast,Chen:2016,DBLP:conf/sigmod/FaridRIHC16}. Given the large and increasing size of KBs, these approaches aggressively prune the search space by focusing on a simple language.
%The memory constraint leads to 
%which leads to very aggressive pruning of the search space 
%More recent approaches also rely on in-memory algorithms and try to solve this problem with distributed architectures~\cite{Chen:2016,DBLP:conf/sigmod/FaridRIHC16}.

%\vspace{1ex}
We present \krd (\underline{Ru}le \underline{Di}scovery in \underline{K}nowledge Bases), a novel system for the robust discovery of positive and negative rules %Horn Rules 
over KBs that addresses these challenges.
\krd is the first system capable of discovering both {\em positive and negative rules} over noisy and incomplete KBs. 
%Such rules are {\em approximate}, i.e., they do not have to hold exactly, since errors and incompleteness are in the nature of KBs. 
Moreover, by relying on disk based algorithms, \krd discovers rules with a richer language that allows value comparisons. This increase in the {\em expressive power} enables a larger number of patterns that can be expressed in the rules, 
and therefore a larger number of new facts and errors that can be identified. %, and with increasing precision. 
These results are achieved by exploiting the following contributions.
%  . We drop such an assumption and propose a disk-based solution that loads into memory only a small portion of the KB. 
%This is a key point to address the size problem, but also because it enables us to enlarge the expressive power of the considered rules to include comparisons among constants, including inequalities.

%\vspace{0.5ex}
\myparagraph{Problem Definition}
We formally define the problem of rule discovery over erroneous and incomplete KBs. 
%%%%%%%%%JOURNAL% The input of the problem consists of a target predicate, from which we identify sets of positive and negative examples.  
In contrast to the traditional ranking of a large set of rules based on a measure of support~\cite{dehaspe1999discovery,galarraga2015fast,schoenmackers2010learning}, our problem definition aims at the identification of a compact subset of approximated rules. % to be exposed to the user. 
Given errors in the data and incompleteness, the ideal solution is a compact set of rules that cover the majority of input positive examples, and as few input negative examples as possible. We map the problem to the %well-known 
weighted set cover problem (Section~\ref{sec:problem}).

%The main benefit of such an approach is to relieve the user from the manual validation of all approximate rules, as they  can be huge in number, or from the definition of a suitable threshold that separates good and bad rules. 
%I REMOVED THIS BECAUSE WE ARE NOT REALLY TOUCHING THIS TOPIC IN THE INTRO, but i mention it in the exp now

%WE MAY WANT TO SAY MORE ABOUT HOW SMART WE ARE BY USING OUR SELECTION with set COVER< BUT DO WE REALLY DO BETTER THAN RANKING (with out own scoring)? I DONT REMEMBER IF WE TESTED THIS
%STEFANO: WELL WE TESTED AGAINST AMIE WHICH IS USING A RANK OF RULES, AND AS RESULT AMIE OUTPUTS MANY RULES, WITH VERY FEW CORRECT. SO YES WE DO BETTER THAN RANKING
%, which is proven to be $\textsf{NP\mbox{-}complete}$.  

%We will show that properly setting such thresholds is not trivial (see Section~\ref{sec:krd_comparative}), and we incorporate this aspect in the problem definition.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\vspace{0.5ex}
\myparagraph{Example Generation}
Positive and negative examples for a target predicate are crucial to our approach as they determine the ultimate quality of the rules. However, crafting a large number of examples is a tedious exercise that requires manual work. 
%%%%%%%%%JOURNAL% Moreover, to effectively steer the algorithm towards useful rules, the input examples must have properties, such as the existence of at least a predicate between pairs of entities in each example. 
We design an algorithm for example generation that is aware of inconsistencies in the KB. Our generated examples lead to better rules than examples obtained with alternative approaches (Section~\ref{sec:rules_gen}). %


%\vspace{0.5ex}
\myparagraph{Rule Discovery Algorithm}
We give a \texttt{log($k$)}-approximation algorithm for the rule discovery problem, % close in spirit to the greedy algorithm for set cover, 
where $k$ is the maximum number of input positive examples covered by a single rule. We discover rules by judiciously using the memory. The algorithm incrementally 
materializes the KB as a graph, and discovers rules by navigating its paths. 
%%%%%%%%%JOURNAL% To reduce the use of resources, at each iteration we follow the most promising path in the $A^*$ traversal fashion. %, pruning unpromising paths.	
%Previous approaches for rules discovery assume that the KB has to fit in main memory to deliver acceptable performance~\cite{abedjan2014amending,galarraga2015fast} (TODO: cite 2 sigmod papers). Unfortunately, KBs nowadays have sizes that can easily exceed hundreds of GBs and previous works require powerful resources to handle them. With our rules generation based on 
By materializing only the portion of the KB that is needed for the discovery, 
%and generating nodes and edges \emph{on demand}, 
the disk is accessed only whenever the navigation of a specific path is required (Section~\ref{sec:krd_greedy}). 
%Moreover, the $A^*$ graph traversal allows the pruning of unpromising paths that are therefore never materialised. Thus 
%The greedy traversal of the search space 
%%%%%%%%%JOURNAL% The greedy algorithm reduces the running time by an order of magnitude in the best case, and its low memory footprint enables the discovery of expressive rules for large KBs. % that do not fit into memory.


%\myparagraph{Sampling for Exploration}
%To further improve performance, we also introduce sampling techniques that are aware of the data challenges in KBs. Our sampling leads to the same improvements in execution times than traditional samplings approach, but, as it is design to mitigate the problems in KBs, it can lead to better rules, i.e., a larger number of correct rules in the output and a smaller number of false rules.
%STEFANO: IS THIS CORRECT? OR DO WE JUST LIMIT THE NUMBER OF FALSE RULES?

%\vspace{0.5ex}
We experimentally test the performance of %rule discovery in 
\krd %%%%%%%%%JOURNAL%by using real-world datasets. The evaluation is carried 
on  three popular and widely used KBs, namely \dbpedia~\cite{bizer2009dbpedia}, \yago~\cite{suchanek2007yago}, and \wikidata~\cite{vrandevcic2014wikidata} (Section~\ref{sec:krd_experiments}). 
We show that our system
%%%%%%%%%JOURNAL%problem formulation together with our 
%that are better in quality than previous systems and that the 
%%%%%%%%%JOURNAL%search algorithm 
delivers accurate rules, 
with a relative increase in average precision by 45\% both in the positive and in the negative settings w.r.t. state-of-the-art systems.
Also, \krd performs consistently well with KBs of all sizes, while other systems cannot scale or fail altogether. % due clearly outperforming state-of-the-art systems . 
%%%%%%%%%JOURNAL% We also 
%show the impact of the proposed optimization and 
%%%%%%%%%JOURNAL% demonstrate how negative rules %have a beneficial impact in providing representative examples to Machine Learning algorithms.
%%%%%%%%%JOURNAL% provide training examples for Machine Learning algorithms of quality comparable to examples manually crafted by humans.
% (Section~\ref{sec:krd_deep_dive}).
%\vspace{-1ex}



