\section{Introduction}
%So far, the focus of this thesis has targeted classic web data extraction: we extract entities from semi-structured web sources, along with their attributes, in order to store them in some structured format. A novel research direction is recently focusing on extracting data from web documents to construct knowledge bases (KBs), in order to structurally store entities and relationships among them. Given the different nature of target data, KBs are usually not stored in relational databases. In this chapter we move the attention from classic web data extraction to KBs built from web sources. More specifically, we consider RDF KBs, the most popular format where data is stored as RDF triples.

Building large RDF knowledge bases (KBs) is a popular trend in information extraction.
KBs store information in the form of triples, where a \emph{predicate}, or relation, expresses a binary relation between a \emph{subject} and a \emph{object}. KB triples, often referred as facts, store information about real-world entities and their relationships, such as %``Scott Eastwood is the child of Clint Eastwood",
 ``Michelle Obama is married to Barack Obama", or ``Larry Page is the founder of Google".
Significant effort has been put in the research community on KBs creation in the last 10 years (\system{DBPedia}~\cite{bizer2009dbpedia}, \system{FreeBase}\cite{bollacker2008freebase}, \system{Wikidata}~\cite{vrandevcic2014wikidata}, \deepdive~\cite{shin2015incremental}, \system{Yago}~\cite{suchanek2008yago}, \system{NELL}~\cite{carlson2010toward}, \system{TextRunner}\cite{banko2007open}) as well as in the industry % and KBs have also attracted interests from the industry, as evident from the ongoing efforts in several companies 
(e.g., Facebook\footnote{\url{https://developers.facebook.com/docs/graph-api}}, Google~\cite{dong2014knowledge}, Wal-Mart~\cite{deshpande2013building}).
%ADD POINTER TO knowledge graph?

Unfortunately, due to their creation process, KBs are usually erroneous and incomplete.
KBs are bootstrapped by extracting information from sources, oftentimes from the Web, with minimal or no human intervention.  This leads to two main problems. First, errors are propagated from the sources, or introduced by the extractors, leading to false facts in the KB. Second, usually KBs do not expose a schema that clearly defines instance data. The set of predicates is unknown a-priori, and adding new predicates/facts is just a matter of inserting new triples in the KB without any schema/integrity check.  
Since \emph{closed world assumption} (CWA) does not hold~\cite{dong2014knowledge,galarraga2015fast}, we cannot assume that a missing fact is false, but rather we should label it as \emph{unknown} (\emph{open world assumption}).


As a direct consequence, the amount of incompleteness and inconsistencies in KBs is significantly larger than classic databases~\cite{suchanek2009sofie}.
Given the abundance of data, KBs are large, e.g., \wikidata has more than $1$ billion facts and $300$ million different entities. %\footnote{\url{https://query.wikidata.org/}}. 
Therefore, tools are needed to assist humans in the curation of the KBs. 
A natural approach to assist curators of KBs is to discover %However, KBs carry such a big amount of information that they can be mined to discover 
\emph{declarative rules}  that can be executed over the KBs to improve the quality of the data, and this is the problem we study in
%inspected to find additional information or constraints over existing data. 
this paper.
% we investigate the problem of discovering \emph{declarative rules} over RDF KBs. 
More specifically, we target the discovery %of first-order Horn Rules constrained by some language biases, introducing 
two different types of rules:
\begin{inparaenum}[\itshape(i)]
	\item {\em positive rules}, used to enrich the KB with new facts and thus increase its coverage of the reality;
	\item {\em negative rules}, used to spot logical inconsistencies and identify erroneous triples.
\end{inparaenum}

\begin{example}\label{ex:krd_intro}
	Consider a KB with information about parent and child relationships.
	A positive rule can be:
	\begin{equation*}
		\atom{parent}{b}{a} \Rightarrow \atom{child}{a}{b}
	\end{equation*}
	which states that if a person $a$ is parent of person $b$, then $b$ is child of $a$. If the KB states that ``Clint Eastwood is the parent of Scott Eastwood", we can infer the fact that ``Scott Eastwood is the child of Clint Eastwood".
	A negative rule has similar form, but very different semantics:
	\begin{equation*}
		\atom{birthDate}{a}{v_0} \wedge \atom{birthDate}{b}{v_1} \wedge v_0 > v_1 \Rightarrow \neg \atom{child}{a}{b} 
	\end{equation*}
	The rule states that a person $b$ cannot be child of $a$ if $a$ was born after $b$. By instantiating the above rule for the $\atom{child}$ predicates in the KB, we may discover erroneous triples stating that a child is born before her parent.
\end{example}

While intuitive to humans, the rules above must be manually stated in a KB in order to be enforced, and there are thousands of rules in a large KB with hundreds of predicates~\cite{gc2015big}. Other than enriching and cleaning KBs, declarative rules brings multiple benefits. %, such as automatic reasoning. 
%We can discover negative facts (absent in KBs), bringing the KB world a step closer to the classic database CWA. 
We will show how negative rules 
%can help domain experts in automatically defining rules to maintain and curate Big Data industrial systems. We 
can support Machine Learning tasks by providing meaningful training examples~\cite{richardson2006markov,shin2015incremental}.

There are been several papers addressing rules discovery in database, and lately in KBs. 
Discovering declarative rules from data sources is a long standing research challenge. Inductive Logic Programming (ILP) was the first discipline to mine Horn Rules from background knowledge and training examples~\cite{muggleton1994inductive}. Similarly, a significant body of work has addressed the problem of discovering constraints over relational databases. The most popular constraint families are Functional Dependencies ~\cite{abiteboul1995foundations,huhtala1999tane,wyss2001fastfds} and Denial Constraints~\cite{chu2013discovering}.
However, this is the first work that address the discovery of both positive and negative rules for KBs, thus facing three main challenges that make hard the discovery of rules.

\myparagraph{Expressive Power}
The more expressive is the rule language, the larger is the number of patterns that can be expressed and the coverage of the facts in the KB. Larger coverage lead to more facts derived and more errors identified. However, it also comes with computational cost, as the search space quickly becomes very large. For this reason, existing approaches for mining KBs~\cite{abedjan2014amending,galarraga2015fast}[{\bf TODO: cite Sigmod 2 papers}] rely on a simple language without values comparisons.

\myparagraph{Errors}
ILP and database techniques usually make the assumption that data is either clean or has a negligible amount of errors. We will show that KBs present a high percentage of errors and we need techniques that are noise tolerant.

\myparagraph{Open Word Assumption}
KBs contain only positive statements, and since we cannot rely on the CWA, we cannot derive negative statements as counter examples --  classic databases approaches rely on the presence of positive and negative examples or on the CWA.  %REFERENCE AND/OR EXPLANATION NEEDED HERE

\myparagraph{Volume}
Classic ILP systems cannot cope with large KBs and in order to directly apply existing database techniques would require the materialisation of all possible predicates combinations into relational tables (graph data must be flattened in relational format).  Moreover, existing approaches for KB assume that data can fit into memory, something that it is not realistic given the large, and increasing, size of the KB.
 

%\myparagraph{Novelty} In this chapter 
\vspace{1ex}
We present \krd (\underline{Ru}les \underline{Di}scovery in \underline{K}nowledge Bases), a novel system for the discovery of positive and negative rules %Horn Rules 
over KBs that address the challenges above.  
\krd is the first system capable of discovering both approximate positive and negative rules and
  % using the same algorithm. Moreover, classic rules discovery systems 
does not assume that the KB fits into memory.
%  . We drop such an assumption and propose a disk-based solution that loads into memory only a small portion of the KB. 
This is a key point to address the size problem, but also because it enables us to enlarge the expressive power of the considered rules to include comparisons among constants, including inequalities.

\begin{itemize}
\item We formally define the problem of rules discovery over erroneous and incomplete KBs. The input of the problem consists of a target predicate, from which we identify sets of positive and negative examples.  In contrast with traditional ranking of rules based on a measure of support~\cite{dehaspe1999discovery,galarraga2015fast,muggleton1995inverse,schoenmackers2010learning}, our  problem definition aims at identification of a subset of useful rules to be exposed to the user. 
Given the presence of errors in the data, the ideal solution is a compact set of rules that cover the majority of the positive examples, and as few negative examples as possible. We map the problem to the well-known weighted set cover problem.

The main benefit of such an approach is to relieve the user from the manual validation of all approximate rules, as they  can be huge in number, or from the definition of a suitable threshold that separates good and bad rules. 
%WE MAY WANT TO SAY MORE ABOUT HOW SMART WE ARE BY USING OUR SELECTION with set COVER< BUT DO WE REALLY DO BETTER THAN RANKING (with out own scoring)? I DONT REMEMBER IF WE TESTED THIS
%, which is proven to be $\textsf{NP\mbox{-}complete}$.  

%We will show that properly setting such thresholds is not trivial (see Section~\ref{sec:krd_comparative}), and we incorporate this aspect in the problem definition.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\item We give a \texttt{log($k$)} approximation of the problem close in spirit to the greedy algorithm for set cover, where $k$ is the maximum number of positive examples covered by a single rule. We discover the rules to feed the algorithm by judiciously using the memory. We incrementally 
%The algorithm we propose in \krd alternates and combines two different phases:
%\begin{inparaenum}[\itshape(i)]
%
materialise the KB as a directed graph, and discovers rules by navigating valid paths on the graph. To reduce the use of resources, at each iteration we follow the most promising path in a $A^*$ traversal fashion, allowing the pruning of unpromising paths.	
%Previous approaches for rules discovery assume that the KB has to fit in main memory to deliver acceptable performance~\cite{abedjan2014amending,galarraga2015fast} (TODO: cite 2 sigmod papers). Unfortunately, KBs nowadays have sizes that can easily exceed hundreds of GBs and previous works require powerful resources to handle them. With our rules generation based on 
This approach materialises only the portion of the KB that is needed for the discovery, generating nodes and edges \emph{on demand} and querying the disk only whenever the navigation of a specific path is required. 
%Moreover, the $A^*$ graph traversal allows the pruning of unpromising paths that are therefore never materialised. Thus 
The significant reduction of the search space reduces the running time an order of magnitude in the best case scenario. 

\item To further improve performance, we also introduce sampling techniques that are aware of the data challenges in KBs. Our sampling leads to the same improvements in execution times than traditional samplings approach, but, as it is design to mitigate the problems in KBs, it can lead to better rules, i.e., a larger number of correct rules in the output and a smaller number of false rules.

\end{itemize}


We experimentally verify the performance of rules discovery in \krd using both real-world datasets (Section~\ref{sec:krd_experiments}). The evaluation is carried on the 3 most popular and widely used KBs, namely \dbpedia~\cite{bizer2009dbpedia}, \yago~\cite{suchanek2007yago}, and \wikidata~\cite{vrandevcic2014wikidata}. We show that our algorithm is general enough to deliver accurate rules both in the positive and negative setting, and it clearly outperforms state-of-the-art systems both in final accuracy and running time. We also give an example of application where our rules discovery approach has a beneficial impact when providing representative examples for Machine Learning algorithms (Section~\ref{sec:krd_deep_dive}).