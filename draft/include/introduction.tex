\section{Introduction}
So far, the focus of this thesis has targeted classic web data extraction: we extract entities from semi-structured web sources, along with their attributes, in order to store them in some structured format. A novel research direction is recently focusing on extracting data from web documents to construct knowledge bases (KBs), in order to structurally store entities and relationships among them. Given the different nature of target data, KBs are usually not stored in relational databases. In this chapter we move the attention from classic web data extraction to KBs built from web sources. More specifically, we consider RDF KBs, the most popular format where data is stored as RDF triples.

Building large RDF KBs has become a popular trend in information extraction.
Differently from classic relational databases, KBs store information in the form of triples, where a \emph{predicate} expresses a binary relation\footnote{We will
	use the two terms ``predicate" and ``relation" interchangeably in this chapter} between a \emph{subject} and a \emph{object}. KB triples, often referred as facts, store information about real-world entities, such as ``Scott Eastwood is the child of Clint Eastwood", ``Michelle Obama is the spouse of Barack Obama", or ``Larry Page is the founder of Google".
The most popular research efforts in this direction can be currently grouped into two strands, according to the creation technique they rely on: \emph{human crafted} KBs (\system{DBPedia}~\cite{bizer2009dbpedia}, \system{FreeBase}\cite{bollacker2008freebase}, \system{Wikidata}~\cite{vrandevcic2014wikidata}), and automated \emph{information extraction} (\deepdive~\cite{shin2015incremental}, \system{Yago}~\cite{suchanek2008yago}, \system{NELL}~\cite{carlson2010toward}, \system{TextRunner}\cite{banko2007open}). 
Recent advances in KBs have also attracted interests from the industrial world, as evident from several companies directions (Facebook Graph\footnote{\url{https://developers.facebook.com/docs/graph-api}}, Google Knowledge Vault~\cite{dong2014knowledge}, Wallmart~\cite{deshpande2013building}).

KBs are built by extracting information from web sources. Given the abundance of web data, KBs nowadays end up being prohibitively large -- the online version of \wikidata contains approximately more than $1$ billion facts and $300$ million different entities\footnote{\url{https://query.wikidata.org/}}. Furthermore, usually KBs do not expose a schema that clearly defines instance data. The set of predicates is unknown a-priori, and adding new predicates/facts is just a matter of inserting new triples in the KB without any schema/integrity check. As a direct consequence, the amount of incompleteness and inconsistencies in KBs is significantly larger than classic databases~\cite{suchanek2009sofie}. For example, the standard \emph{closed world assumption} (CWA) does not longer hold in KBs~\cite{dong2014knowledge,galarraga2015fast}. We cannot assume that a fact that is not stated in a KB is false, but rather we should label it as \emph{unknown}. This is known as \emph{open world assumption} (OWA).

Despite being erroneous and incomplete, KBs carry such a big amount of information that they can be inspected to find additional information or constraints over existing data. In this chapter we investigate the problem of discovering \emph{declarative rules} over RDF KBs. More specifically, we target the discovery of first-order Horn Rules constrained by some language biases. We define two different types of rules:
\begin{inparaenum}[\itshape(i)]
	\item positive rules, used to enrich the KB with new facts and thus make it more complete;
	\item negative rules, used to spot logical inconsistencies and repair erroneous triples.
\end{inparaenum}

\begin{example}\label{ex:krd_intro}
	As an example, consider a KB that contains information about parent and child relationships.
	A potential positive rule could be:
	\begin{equation*}
		\atom{parent}{b}{a} \Rightarrow \atom{child}{a}{b}
	\end{equation*}
	which states that if $a$ is parent of $b$, then $b$ is child of $a$. If the KB states that ``Clint Eastwood is the parent of Scott Eastwood", we can infer the fact that ``Scott Eastwood is the child of Clint Eastwood".
	On the other hand, a negative rule may state that:
	\begin{equation*}
		\atom{birthDate}{a}{v_0} \wedge \atom{birthDate}{b}{v_1} \wedge v_0 > v_1 \Rightarrow \neg \atom{child}{a}{b} 
	\end{equation*}
	The above rule expresses the concept that $b$ cannot be child of $a$ if $a$ was born after $b$. By instantiating the above rule on the KB, we may discover erroneous triples where a child is born before her parent.
\end{example}

Other than enriching and cleaning KBs, discovering declarative rules brings multiple benefits. We can discover negative facts (absent in KBs), bringing the KB world a step closer to the classic database CWA. We can help domain experts in automatically defining rules to maintain and curate Big Data industrial systems~\cite{gc2015big}. We can enhance Machine Learning tasks by providing meaningful training examples~\cite{richardson2006markov,shin2015incremental}.

Discovering declarative rules from data sources is a long standing research challenge. Inductive Logic Programming (ILP) was the first discipline to mine Horn Rules from background knowledge and training examples~\cite{muggleton1994inductive}. Similarly, a significant body of work has addressed the problem of discovering constraints over relational databases. The most popular constraint families are Functional Dependencies ~\cite{abiteboul1995foundations,huhtala1999tane,wyss2001fastfds} and Denial Constraints~\cite{chu2013discovering}. However, current KBs pose new challenges that make these approaches unsuitable. First, the size of modern KBs. Classic ILP systems cannot cope with large KBs and adopting database techniques would require the materialisation of all possible predicates combinations into relational tables. Second, the OWA that rules KBs. KBs contain only positive statements, and since we cannot rely on the CWA, we cannot derive negative statements as counter examples -- most of classic databases approaches rely on the presence of positive and negative examples or on the CWA. Third, ILP and database techniques usually make the assumption that data is either clean or has a negligible amount of errors. We will show that KBs present a high percentage of errors and we need techniques that are noise tolerant.


\myparagraph{Novelty} In this chapter we present \krd (\underline{Ru}les \underline{Di}scovery in \underline{K}nowledge Bases), a novel system for the discovery of positive and negative Horn Rules over KBs. To the best of our knowledge, \krd is the first system capable of discovering both positive and negative rules using the same algorithm. Moreover, classic rules discovery systems assume that the KB has to fit in memory to have acceptable performance~\cite{abedjan2014amending,galarraga2015fast} (TODO: cite Sigmod 2 papers). We drop such an assumption and propose a disk-based solution that loads into memory only a small portion of the KB. This is a key point not only because we can discover rules on any size KB, but also because we can enlarge the expressive power of the considered rules to include smart comparisons among constants and numbers, including inequalities.

\myparagraph{Contribution 1} 
We first formally define the problem of rules discovery over erroneous and incomplete KBs. The input of the problem consists of a target predicate, uniquely identified by a set of positive and negative examples. The ideal solution of the problem is a set of rules that cover all positive examples, and as few negative examples as possible. The problem can be naturally mapped to the well-known weighted set cover problem, which is proven to be $\textsf{NP\mbox{-}complete}$.  

In contrast with traditional ranking of rules based on a measure of support~\cite{dehaspe1999discovery,galarraga2015fast,muggleton1995inverse,schoenmackers2010learning}, our novel problem definition inspired by set cover allows the identification of a subset of useful rules to be exposed to the user. The main benefit of such an approach is to relieve the user from the definition of a suitable threshold that separates good and bad rules. We will show that properly setting such thresholds is not trivial (see Section~\ref{sec:krd_comparative}), and we incorporate this aspect in the problem definition.


\myparagraph{Contribution 2} We give a \texttt{log($k$)} approximation of the problem close in spirit to the greedy algorithm for set cover. $k$ is the maximum number of positive examples covered by a single rule. The algorithm we propose in \krd alternates and combines two different phases:
\begin{inparaenum}[\itshape(i)]
	\item it materialises the KB as a directed graph and discovers rules by navigating valid paths on the graph;
	\item at each iteration it follows the most promising path in a $A^*$ traversal fashion, allowing the pruning of unpromising paths.	
\end{inparaenum}

Previous approaches for rules discovery assume that the KB has to fit in main memory to deliver acceptable performance~\cite{abedjan2014amending,galarraga2015fast} (TODO: cite 2 sigmod papers). Unfortunately, KBs nowadays have sizes that can easily exceed hundreds of GBs and previous works require powerful resources to handle them. With our rules generation based on graph traversal we materialise only a small portion of the KB that is needed for the discovery, generating nodes and edges \emph{on demand} by querying the disk whenever the navigation of a specific path is required. Moreover, the $A^*$ graph traversal allows the pruning of unpromising paths that are therefore never materialised. Thus we significantly reduce the search space, cutting down the running time of ten times in the best case scenario. 

\myparagraph{Contribution 3}
We experimentally verify the performance of rules discovery in \krd using both real-world and synthetic datasets (Section~\ref{sec:krd_experiments}). The evaluation is carried on the 3 most popular and widely used KBs, namely \dbpedia~\cite{bizer2009dbpedia}, \yago~\cite{suchanek2007yago}, and \wikidata~\cite{vrandevcic2014wikidata}. We show that our algorithm is general enough to deliver accurate rules both in the positive and negative setting, and it clearly outperforms state-of-the-art systems both in final accuracy and running time. Eventually we give an example of application where our rules discovery approach gives a beneficial impact, providing representative examples for Machine Learning algorithms (Section~\ref{sec:krd_deep_dive}).