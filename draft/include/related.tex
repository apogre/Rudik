\section{Related Work} \label{sec:krd_related}

Our work is inspired by Inductive Logic Programming, but uses techniques from dependencies discovery in relational databases, KBs construction, and mining graph patterns. 
We review a few of the most relevant works below.

%\myparagraph{Relational Database Constraints} 
A significant body of work has addressed the problem of discovering constraints over relational database sources. 
Due to the presence of a pre-defined schema in relational databases, constraints are enforced based on the the dependencies among the attributes by well-studied formalisms such as 
Functional Dependencies (FDs)(~\cite{abiteboul1995foundations},~\cite{abedjan2014dfd},~\cite{huhtala1999tane},~\cite{wyss2001fastfds}), Conditional FDs (~\cite{fan2011discovering}) 
and Denial Constraints (DCs)~\cite{bertossi2011database,chu2013holistic}. It is interesting to note that the positive rules we discover using \krd carry 
a semantic resemblance to FDs while 
a close parallel can be drawn between our negative rules and DCs.
%
%Functional Dependencies (FDs), a formalism to express relations and dependencies among attributes, has been studied in constraints discovery literature for more than 20 years~\cite{abiteboul1995foundations}, with a recent focus on performance~\cite{abedjan2014dfd}. FDs can be grouped into two strands: the schema-level approaches~\cite{huhtala1999tane} (similar to our positive rules discovery), and the instance-driven approaches~\cite{wyss2001fastfds}. More recently, Conditional FDs extend standard FDs by enforcing patterns of semantically related constants~\cite{fan2011discovering}. In the context of inconsistencies discovery for relational data, Denial Constrains (DCs) are the current state-of-the-art techniques~\cite{chu2013discovering}. DCs are a universally quantified first order logic formalism to express constraints over relational data, and they are directly related to our negative output rules. Efficient DCs algorithms have been proposed for data cleaning and consistent query answering~\cite{bertossi2011database,chu2013holistic}. 

Despite being directly related to our output and expressing a richer language, FDs and DCs cannot be applied to RDF databases for three main reasons:
\begin{inparaenum}[\itshape(i)]
	\item the schema-less nature of RDF data and the closed world assumption which no longer holds on RDF KBs;
	\item FDs and DCs rely on the assumption that data is either clean or has a negligible amount of errors;
	\item scalability issues on large RDF dataset: applying relational database techniques on RDF KBs would imply materializing all possible predicate combinations into relational tables.
\end{inparaenum}

A few FD mining approaches like~\cite{abedjan2015temporal,kivinen1995approximate} that can work with erroneous datasets are still inapplicable to RDF data due to scalability problems.
Fan et. al.~\cite{FanFDGraphs} laid the theoretical foundations of Functional Dependencies on Graphs (GFDs). They also propose parallel algorithms for GFDs computation and evaluate the accuracy on \yago and \dbpedia. Despite existing a natural correlation between FDs and Horn Rules, the language they propose covers only a portion of our negative rules to detect inconsistencies in graph databases. Moreover their language does not include smart literals comparisons, shown to be vital when detecting errors in KBs.

On the other hand, rule mining approaches specifically aimed towards RDF KBs such as \amie~\cite{galarraga2015fast} and OP algorithm~\cite{Chen:2016} load the entire KB into memory prior to 
the graph traversal step connecting the predicates incrementally to form the full-fledged rule. In addition to this, neither of these two approaches can afford smart literal comparison. 
In contrast to them, \krd is disk-based and generates the graph on-demand and can discover rules on a small fraction of the KB examples that not only makes our approach scalable but also helps us 
accommodate smart literal comparisons from a language perspective. We have not tested our running time against~\cite{Chen:2016} because we could not find an implementation of it, 
however it requires a powerful cluster of several machines to split the KB into multiple nodes. 
We showed in the experimental section how \krd outperforms \amie both in final accuracy and running time.

Our example generation strategy leverages on the Local Closed World Assumption (LCWA). When dealing with incomplete KBs, the LCWA is a popular technique that replaces the canonical Closed World Assumption of standard relational databases. The LCWA has been used in Google Knowledge Vault to estimate the quality of extracted triples~\cite{dong2014data,dong2014knowledge}, \amie~\cite{galarraga2015fast} uses the LCWA to penalise discovered rules, and sometimes the LCWA is used to evaluate the quality of a target KB~\cite{dong2015knowledge}. We see our examples generation strategy as complementary to our approach. It is possible to run \krd with any input examples, no matter how such examples have been generated.

%\myparagraph{KBs Rule Mining} Recently, the focus for constraints and rules discovery is moving towards RDF databases. The closest works to ours are \amie~\cite{galarraga2015fast} and OP algorithm (TODO: cite Ontological Path Finding), which discover positive Horn Rules from RDF KBs with same language biases. They both uses the same discovery algorithm: it first loads the entire KB into memory and then, working one predicate at time, tries to expand rules by connecting a predicate to others that share common variables. Rules are ranked according to a confidence measure that leverages on KBs partial closed world assumption. Our graph generation and navigation technique is similar to their approach, however our examples generation allows us to discover rules on just a small fraction of the KB. This is beneficial not only from a scalability point of view (see Section~\ref{sec:krd_comparative}), but also from the language perspective: neither of the two approaches can afford smart literals comparisons. We have not tested our running time against (TODO: cite Ontological Path Finding) because we could not find an implementation of it, however it requires a powerful cluster of several machines to split the KB into multiple nodes. We showed in the experimental section how \krd outperforms \amie both in final accuracy and running time.

\cite{abedjan2014amending} is an instance-level approach that discovers new facts for specific entities rather than generic variables contrary to which the rules generated by \krd are 
generic and can be instantiated with several instances thus being capable of generating several highly precise facts.~\cite{DBLP:conf/sigmod/FaridRIHC16} is a modern system to discover Conditional Denial Constraints (CDCs) from RDF Data. Differently from other systems, it includes literals in its language. CDCs can be directly mapped to our negative rules, however there is not a general correlation between CDCs and positive rules. Another major difference of
our setting is the hardware: our disk-based approach is designed to handle large KBs with limited memory resources, while ~\cite{DBLP:conf/sigmod/FaridRIHC16} works on a distributed environment with a total of 832 GB RAM memory.

To the best of our knowledge, \krd is the first approach that is generic enough to use the same algorithm to discover both positive and negative rules in RDF KBs.

%\myparagraph{Inductive Logic Programming} 
Inductive Logic Programming (ILP) is a sub-field of Machine Learning and Logic Programming which investigates the inductive construction of first-order Horn Rules from examples and background knowledge, usually expressed through logic formalisms~\cite{muggleton1994inductive}. \krd can be seen as an ILP system where the KB is the background knowledge, and the generation and validation sets correspond to positive and negative training examples.

%\system{WARMR} is an ILP system that discovers frequent patterns (expressed through DATALOG queries) that succeed with respect to a sufficient number of examples~\cite{dehaspe1999discovery}. When translated to databases, such patterns correspond to conjunctive queries. \system{ALEPH}\footnote{\url{http://www.cs.ox.ac.uk/activities/machinelearning/Aleph/aleph}} is an available ILP system that is based on Prolog Inverse Entailment~\cite{muggleton1995inverse}. \system{ALEPH} works iteratively, by selecting examples from the background knowledge. It first constructs the most specific clause that entails the example selected (\emph{bottom clause}), and then searches for some subset of the literals in the bottom clause that has the \emph{best score} in order to define more general rules. The system allows the user to choose among several scoring functions. 

ILP systems such as \system{WARMR} and \system{ALEPH}\footnote{\url{http://www.cs.ox.ac.uk/activities/machinelearning/Aleph/aleph}} are designed to work with a closed world assumption, 
and always require the definition of positive and negative examples. \amie clearly outperfroms these two systems~\cite{galarraga2015fast}, showing evident scalability issues 
when dealing medium-size KBs. Moreover, one of the main limitations of classic ILP systems is the assumption of having high-quality, errors-free training examples. 
We showed how this assumption does not longer hold on incomplete and erroneous KBs. \system{Sherlock}~\cite{schoenmackers2010learning} is an ILP system that extracts first-order Horn Rules 
from Web text. While making \krd extensible to free text rather than relying on a well-defined KB can be seen as an interesting future work, 
the statistical significance estimates that is used by 
\system{Sherlock} need a threshold to discover meaningful rules. Several ILP systems inspired from Association Rule Mining~\cite{agrawal1993mining} also use thresholds for 
support and confidence which are non-trivial to set as mentioned in Section~\ref{sec:krd_comparative}. We avoid the notion of a threshold \krd relies on a scoring function that simulates the set cover problem and outputs rules only when coverages over generation and validation sets are considered acceptable.

%\system{Sherlock}~\cite{schoenmackers2010learning} is an interesting ILP system that learns first-order Horn Rules from Web text. One of the key advantage of \system{Sherlock} is being unsupervised: it does not require negative training examples. It uses statistical significance and statistical relevance in order to discover rules that exceeds a given threshold. Differently from our setting, \system{Sherlock} is specifically designed to learn Horn Rules from open domain facts that are extracted from the Web. An interesting future direction is to adapt \krd to discover rules from relations extracted from free text rather than on a well-defined KB.

%ILP systems take inspiration from Association Rule Mining~\cite{agrawal1993mining}, where given a database of costumer transactions the goal is to discover all frequent itemsets, i.e., all combinations of items that are found together with some minimum confidence. A well known example in a supermarket database could state that 90\% of transactions that purchase bread and butter also purchase milk. As previously mentioned, adapting such a relational database setting to KBs would require the materialisation of all possible predicates combinations into relational tables.

%Eventually, most of the ILP and KBs rules discovery systems rank rules according to a support value, and output only those rules that exceed a given threshold. We showed in Section~\ref{sec:krd_comparative} that properly setting such thresholds is not trivial, as often good rules are preceded in rank by meaningless ones. \krd does not use any threshold and outputs rules only when coverages over generation and validation sets are considered acceptable.

%\myparagraph{Relation to other areas} 

%Our graph-based rules discovery approach is close in spirit to mining graph patterns~\cite{elseidy2014grami,zou2009distance}, where given a (big) input graph the goal is to discover the most frequent patterns (subgraphs) according to some scoring functions. Our setting presents a key difference: we primary look at edge labels and we are not interested in node labels, since nodes are mapped to variables when translating subgraphs to Horn Rules. Moreover, given the portion of the graph between two entities $x$ and $y$, we are interested in discovery \emph{all} possible subgraphs between $x$ and $y$, which makes the problem easily solvable with BFS-like techniques.

%Another interesting setting is the one of \system{SpiderMine}~\cite{zhu2011mining}. \system{SpiderMine} looks for the top-$K$ largest patterns in a graph, where each node in the pattern is at most at distance $r$ from a head vertex $u$. In our setting we do not have a single head vertex, but rather many vertexes (starting nodes) from which we begin the path computation. Furthermore our goal is not to find graph patterns (subgraphs), but we are simply interested in all possible paths between a pair of vertexes.

