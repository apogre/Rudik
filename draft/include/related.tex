\section{Related Work} \label{sec:krd_related}

Our work %is inspired by Inductive Logic Programming, but 
uses techniques from dependencies discovery in relational databases, KBs construction, and mining graph patterns. 
We review some of the most relevant works below.

%\myparagraph{Relational Database Constraints} 
A significant body of work has addressed the problem of discovering constraints over relational data. 
Due to the presence of a pre-defined schema, dependencies are discovered over the attributes and encoded into formalisms such as 
Functional Dependencies (FDs)(~\cite{abiteboul1995foundations,huhtala1999tane,wyss2001fastfds}), Conditional FDs (~\cite{fan2011discovering}) 
and Denial Constraints (DCs)~\cite{chu2013discovering}. 
%
%It is interesting to note that the Our positive rules  to FDs while a close parallel can be drawn between our negative rules and DCs.
%
%Functional Dependencies (FDs), a formalism to express relations and dependencies among attributes, has been studied in constraints discovery literature for more than 20 years~\cite{abiteboul1995foundations}, with a recent focus on performance~\cite{abedjan2014dfd}. FDs can be grouped into two strands: the schema-level approaches~\cite{huhtala1999tane} (similar to our positive rules discovery), and the instance-driven approaches~\cite{wyss2001fastfds}. More recently, Conditional FDs extend standard FDs by enforcing patterns of semantically related constants~\cite{fan2011discovering}. In the context of inconsistencies discovery for relational data, Denial Constrains (DCs) are the current state-of-the-art techniques~\cite{chu2013discovering}. DCs are a universally quantified first order logic formalism to express constraints over relational data, and they are directly related to our negative output rules. Efficient DCs algorithms have been proposed for data cleaning and consistent query answering~\cite{bertossi2011database,chu2013holistic}. 
%Despite being directly related to our output and expressing a richer language, 
However, techniques for FDs and DCs discovery cannot be applied to RDF databases for three main reasons:
\begin{inparaenum}[\itshape(i)]
	\item the schema-less nature of RDF data and the open world assumption; % which no longer holds on RDF KBs;
	\item traditional approaches rely on the assumption that data is either clean or has a negligible amount of errors, which is not the case with KBs;
	\item even when the algorithms are designed to support more errors~\cite{abedjan2015temporal,kivinen1995approximate}, scalability issues on large RDF dataset: a direct application of relational database techniques on RDF KBs requires the materilization of all possible predicate combinations into relational tables.
\end{inparaenum}

%A few FD mining approaches like~\cite{abedjan2015temporal,kivinen1995approximate} that can work with erroneous datasets are still inapplicable to RDF data due to scalability problems.
Recently, Fan et. al.~\cite{FanFDGraphs} laid the theoretical foundations of Functional Dependencies on Graphs (GFDs). 
%They also propose parallel algorithms for GFDs computation and evaluate the accuracy on \yago and \dbpedia. Despite existing a natural correlation between FDs and Horn Rules, 
However, the language they propose covers only a portion of our negative rules to detect inconsistencies and does not include smart literals comparisons, which we have shown to be useful when detecting errors in KBs.

To the best of our knowledge, \krd is the first approach that is generic enough %to use the same algorithm 
to discover both positive and negative rules in RDF KBs.
Rule mining approaches specifically designed for positive rule discovery in RDF KBs, such as \amie~\cite{galarraga2015fast} and OP algorithm~\cite{Chen:2016}, load the entire KB into memory prior to 
the graph traversal step. %connecting the predicates incrementally to form the full-fledged rule. In addition to this, 
This is a strong constraints for their applicability over large KBs, and neither of these two approaches can afford smart literal comparison. 
In contrast to them, \krd is disk-based. By generating the graph on-demand, it can discover rules on a small fraction of the KB examples. This makes our approach scalable and the low memory footprint enables a bigger search space with rules that can have literal comparisons. % from a language perspective. 
%however it requires a powerful cluster of several machines to split the KB into multiple nodes. 
We showed in the experimental section how \krd outperforms \amie both in final accuracy and running time.
In contrast with recent approaches~\cite{DBLP:conf/sigmod/FaridRIHC16}, our algorithm was designed to run on a single node in a commodity machine. We therefore have not tested our running time against a distributed environment. %~\cite{Chen:2016} because we could not find an implementation for it and 
Finally, \cite{abedjan2014amending} recommends new facts to be added to the KB by using association rule mining techniques. Their rules are made only of constants and are therefore less general than the rules generated by \krd.
% are generic and can be instantiated with several instances thus being capable of generating several highly precise facts.

%Our example generation strategy leverages the Local Closed World Assumption (LCWA) to handle incompleteness in KBs, as done in previous works~\cite{dong2014data,dong2015knowledge}, \amie~\cite{Chen:2016,galarraga2015fast}.
%. LCWA has been used in Google Knowledge Vault
%~\cite{dong2014data,dong2015knowledge}, \amie~\cite{galarraga2015fast} and helps us run with input example set of all sizes.
%Our example generation strategy leverages on the Local Closed World Assumption (LCWA). When dealing with incomplete KBs, the LCWA is a popular technique that replaces the canonical Closed World Assumption of standard relational databases. The LCWA has been used in Google Knowledge Vault to estimate the quality of extracted triples~\cite{dong2014data,dong2014knowledge}, \amie~\cite{galarraga2015fast} uses the LCWA to penalise discovered rules, and sometimes the LCWA is used to evaluate the quality of a target KB~\cite{dong2015knowledge}. We see our examples generation strategy as complementary to our approach. It is possible to run \krd with any input examples, no matter how such examples have been generated.

%\myparagraph{KBs Rule Mining} Recently, the focus for constraints and rules discovery is moving towards RDF databases. The closest works to ours are \amie~\cite{galarraga2015fast} and OP algorithm (TODO: cite Ontological Path Finding), which discover positive Horn Rules from RDF KBs with same language biases. They both uses the same discovery algorithm: it first loads the entire KB into memory and then, working one predicate at time, tries to expand rules by connecting a predicate to others that share common variables. Rules are ranked according to a confidence measure that leverages on KBs partial closed world assumption. Our graph generation and navigation technique is similar to their approach, however our examples generation allows us to discover rules on just a small fraction of the KB. This is beneficial not only from a scalability point of view (see Section~\ref{sec:krd_comparative}), but also from the language perspective: neither of the two approaches can afford smart literals comparisons. We have not tested our running time against (TODO: cite Ontological Path Finding) because we could not find an implementation of it, however it requires a powerful cluster of several machines to split the KB into multiple nodes. We showed in the experimental section how \krd outperforms \amie both in final accuracy and running time.

%~\cite{DBLP:conf/sigmod/FaridRIHC16} is a modern system to discover Conditional Denial Constraints (CDCs) from RDF Data. Differently from other systems, it includes literals in its language. CDCs can be directly mapped to our negative rules, however there is not a general correlation between CDCs and positive rules. Another major difference of our setting is the hardware: our disk-based approach is designed to handle large KBs with limited resources, while ~\cite{DBLP:conf/sigmod/FaridRIHC16} works on a distributed environment with a total of 832 GB RAM memory.


%\myparagraph{Inductive Logic Programming} 
%Inductive Logic Programming (ILP) is a sub-field of Machine Learning and Logic Programming which investigates the inductive construction of first-order Horn Rules from examples and background knowledge, usually expressed through logic formalisms~\cite{muggleton1994inductive}. \krd can be seen as an ILP system where the KB is the background knowledge, and the generation and validation sets correspond to positive and negative training examples.

%\system{WARMR} is an ILP system that discovers frequent patterns (expressed through DATALOG queries) that succeed with respect to a sufficient number of examples~\cite{dehaspe1999discovery}. When translated to databases, such patterns correspond to conjunctive queries. \system{ALEPH}\footnote{\url{http://www.cs.ox.ac.uk/activities/machinelearning/Aleph/aleph}} is an available ILP system that is based on Prolog Inverse Entailment~\cite{muggleton1995inverse}. \system{ALEPH} works iteratively, by selecting examples from the background knowledge. It first constructs the most specific clause that entails the example selected (\emph{bottom clause}), and then searches for some subset of the literals in the bottom clause that has the \emph{best score} in order to define more general rules. The system allows the user to choose among several scoring functions. 

ILP systems such as \system{WARMR}~\cite{dehaspe1999discovery} and \system{ALEPH}\footnote{\url{https://www.cs.ox.ac.uk/activities/machinelearning/Aleph/aleph}} are designed to work under the closed world assumption and require the definition of positive and negative examples. \amie outperforms these two systems~\cite{galarraga2015fast}, but shows scalability issues when dealing medium-size KBs. Moreover, %one of the main limitations of 
classic ILP systems assume high-quality, error-free training examples as input. 
We showed how this assumption does not hold in KBs. \system{Sherlock}~\cite{schoenmackers2010learning} is an ILP system that extracts first-order Horn Rules 
from Web text. While making \krd extensible to free text rather than relying on a well-defined KB is an interesting future work, 
the statistical significance estimate used by 
\system{Sherlock} needs a threshold to discover meaningful rules. Several ILP systems inspired from Association Rule Mining~\cite{agrawal1993mining} also use thresholds for support and confidence that are non-trivial to set (Section~\ref{sec:krd_comparative}). We avoid the use of a threshold in \krd and rely on a weighted set cover problem formulation that outputs only rules contributing to the coverage of the generation set while minimizing the coverage of the validation set.

%\system{Sherlock}~\cite{schoenmackers2010learning} is an interesting ILP system that learns first-order Horn Rules from Web text. One of the key advantage of \system{Sherlock} is being unsupervised: it does not require negative training examples. It uses statistical significance and statistical relevance in order to discover rules that exceeds a given threshold. Differently from our setting, \system{Sherlock} is specifically designed to learn Horn Rules from open domain facts that are extracted from the Web. An interesting future direction is to adapt \krd to discover rules from relations extracted from free text rather than on a well-defined KB.

%ILP systems take inspiration from Association Rule Mining~\cite{agrawal1993mining}, where given a database of costumer transactions the goal is to discover all frequent itemsets, i.e., all combinations of items that are found together with some minimum confidence. A well known example in a supermarket database could state that 90\% of transactions that purchase bread and butter also purchase milk. As previously mentioned, adapting such a relational database setting to KBs would require the materialisation of all possible predicates combinations into relational tables.

%Eventually, most of the ILP and KBs rules discovery systems rank rules according to a support value, and output only those rules that exceed a given threshold. We showed in Section~\ref{sec:krd_comparative} that properly setting such thresholds is not trivial, as often good rules are preceded in rank by meaningless ones. \krd does not use any threshold and outputs rules only when coverages over generation and validation sets are considered acceptable.

%\myparagraph{Relation to other areas} 

%Our graph-based rules discovery approach is close in spirit to mining graph patterns~\cite{elseidy2014grami,zou2009distance}, where given a (big) input graph the goal is to discover the most frequent patterns (subgraphs) according to some scoring functions. Our setting presents a key difference: we primary look at edge labels and we are not interested in node labels, since nodes are mapped to variables when translating subgraphs to Horn Rules. Moreover, given the portion of the graph between two entities $x$ and $y$, we are interested in discovery \emph{all} possible subgraphs between $x$ and $y$, which makes the problem easily solvable with BFS-like techniques.

%Another interesting setting is the one of \system{SpiderMine}~\cite{zhu2011mining}. \system{SpiderMine} looks for the top-$K$ largest patterns in a graph, where each node in the pattern is at most at distance $r$ from a head vertex $u$. In our setting we do not have a single head vertex, but rather many vertexes (starting nodes) from which we begin the path computation. Furthermore our goal is not to find graph patterns (subgraphs), but we are simply interested in all possible paths between a pair of vertexes.

